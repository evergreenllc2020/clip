{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semantic Search on images with OpenAI CLIP - Unsplash.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evergreenllc2020/clip/blob/main/Semantic_Search_on_images_with_OpenAI_CLIP_Unsplash.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPHN7PJgKOzb"
      },
      "source": [
        "#<h2 align=center> Semantic Keyword Search using CLIP </h2> ###\n",
        "### This notebook shows how to fine tune ranking of image search results using CLIP models.\n",
        "### Author: Evergreen Technologies LLC\n",
        "### Linked in Profile : https://www.linkedin.com/in/evergreen-technologies-usa-3a7422198/\n",
        "### Link to Full Udemy Course : https://www.udemy.com/course/build-movie-review-classification-with-bert-and-tensorflow/learn/lecture/24635958#overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53N4k0pj_9qL"
      },
      "source": [
        "# Preparation for Colab\n",
        "\n",
        "Make sure you're running a GPU runtime; if not, select \"GPU\" as the hardware accelerator in Runtime > Change Runtime Type in the menu. The next cells will print the CUDA version of the runtime if it has a GPU, and install PyTorch 1.7.1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUUDdvPNzmzT"
      },
      "source": [
        "<div align=\"center\">\n",
        "    <img width=\"512px\" src='https://docs.google.com/drawings/d/e/2PACX-1vSaom5DqkOZYNGZVoSaliUTYJf_OSz1xsRSRgAldk-TyfqgG5hWyOCjmsyra6z7uwsLCpK6WZZc-dFM/pub?w=960&h=720' />\n",
        "    <p style=\"text-align: center;color:gray\">Figure 1: Overall workflow</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1022TgAQzycB"
      },
      "source": [
        "In this [project](https://www.udemy.com/), you will learn how to fine-tune image search results using CLIP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0MO1G9XzzuM"
      },
      "source": [
        "### Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnuWoUewz2oX"
      },
      "source": [
        "By the time you complete this project, you will be able to:\n",
        "\n",
        "- Tokenize and Preprocess Text for image search\n",
        "- Use UInsplash image Search API to view results prior to fune tuning\n",
        "- Fine tune image search results by encdoing search keyword and image embeddeing using CLIP\n",
        "- Perform similarity between imnage anf text embeddings and select top 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdjxOc2Hz5qe"
      },
      "source": [
        "### Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOZTbCTMz8L9"
      },
      "source": [
        "In order to be successful with this project, it is assumed you are:\n",
        "\n",
        "- Understanding the Python programming language\n",
        "- Basic Familiar with deep learning  \n",
        "- Familiar with Pytorch "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5kDsR620BP_"
      },
      "source": [
        "This project/notebook consists of several Tasks.\n",
        "\n",
        "- **[Task 1]()**: Introduction to the Project.\n",
        "- **[Task 2]()**: Setup your Pytorch and Colab Runtime\n",
        "- **[Task 3]()**: Download pretrained CLIP model\n",
        "- **[Task 4]()**: Download recall image set from Unsplash\n",
        "- **[Task 5]()**: Tokenize search keywords into text embeddings\n",
        "- **[Task 6]()**: Tokenize each image in recall set into image embeddings\n",
        "- **[Task 7]()**: Perform similarity between image embeddings and text embeddings\n",
        "- **[Task 8]()**: Sort the results by descending order similarity score\n",
        "- **[Task 9]()**: Display Top N results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdgmH7t-eOc1"
      },
      "source": [
        "### Task 1 : Introduction to the semantic keywrd image search\n",
        "<div align=\"center\">\n",
        "    <img width=\"512px\" src='https://drive.google.com/uc?id=1fnJTeJs5HUpz7nix-F9E6EZdgUflqyEu' />\n",
        "    <p style=\"text-align: center;color:gray\">Figure 1: Overall workflow Model</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GRp8poledNm"
      },
      "source": [
        "### Task 2: Set up Pytorch and Colab runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azFt0PnV0IpX"
      },
      "source": [
        "You will only be able to use the Colab Notebook after you save it to your Google Drive folder. Click on the File menu and select “Save a copy in Drive…\n",
        "\n",
        "![Copy to Drive](https://drive.google.com/uc?id=1CH3eDmuJL8WR0AP1r3UE6sOPuqq8_Wl7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHfwf5zV0Kap"
      },
      "source": [
        "### Check GPU Availability\n",
        "\n",
        "Check if your Colab notebook is configured to use Graphical Processing Units (GPUs). If zero GPUs are available, check if the Colab notebook is configured to use GPUs (Menu > Runtime > Change Runtime Type).\n",
        "\n",
        "![Hardware Accelerator Settings](https://drive.google.com/uc?id=1qrihuuMtvzXJHiRV8M7RngbxFYipXKQx)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVR6u3Je0WQm"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD4AU7Zr0dyK"
      },
      "source": [
        "### Derive Pytorch version thats compatible with GPU Runtime (CUDA)\n",
        "\n",
        "Check if your Colab notebook is configured to use Graphical Processing Units "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BpdJkdBssk9"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz2Ld2ij2Gnx"
      },
      "source": [
        "### Install Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X8y4LnWzlhs"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBVr18E5tse8"
      },
      "source": [
        "! pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_0NhBL92loA"
      },
      "source": [
        "### import numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1hkDT38hSaP"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFxgLV5HAEEw"
      },
      "source": [
        "### Task 3: Downloading the CLIP model\n",
        "\n",
        "CLIP models are distributed as TorchScript modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLFS29hnhlY4"
      },
      "source": [
        "MODELS = {\n",
        "    \"ViT-B/32\":       \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cboKZocQlSYX"
      },
      "source": [
        "! wget {MODELS[\"ViT-B/32\"]} -O model.pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBRVTY9lbGm8"
      },
      "source": [
        "model = torch.jit.load(\"model.pt\").cuda().eval()\n",
        "input_resolution = model.input_resolution.item()\n",
        "context_length = model.context_length.item()\n",
        "vocab_size = model.vocab_size.item()\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21slhZGCqANb"
      },
      "source": [
        "# Image Preprocessing\n",
        "\n",
        "We resize the input images and center-crop them to conform with the image resolution that the model expects. Before doing so, we will normalize the pixel intensity using the dataset mean and standard deviation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6cpiIFHp9N6"
      },
      "source": [
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from PIL import Image\n",
        "\n",
        "preprocess = Compose([\n",
        "    Resize(input_resolution, interpolation=Image.BICUBIC),\n",
        "    CenterCrop(input_resolution),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "image_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n",
        "image_std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwSB5jZki3Cj"
      },
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "We use a case-insensitive tokenizer. The tokenizer code is hidden in the second cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGom156-i2kL"
      },
      "source": [
        "! pip install ftfy regex\n",
        "! wget https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz -O bpe_simple_vocab_16e6.txt.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toGtcd-Ji_MD"
      },
      "source": [
        "#@title\n",
        "\n",
        "import gzip\n",
        "import html\n",
        "import os\n",
        "from functools import lru_cache\n",
        "\n",
        "import ftfy\n",
        "import regex as re\n",
        "\n",
        "\n",
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
        "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
        "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
        "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
        "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8+n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "\n",
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def basic_clean(text):\n",
        "    text = ftfy.fix_text(text)\n",
        "    text = html.unescape(html.unescape(text))\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def whitespace_clean(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "class SimpleTokenizer(object):\n",
        "    def __init__(self, bpe_path: str = \"bpe_simple_vocab_16e6.txt.gz\"):\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n",
        "        merges = merges[1:49152-256-2+1]\n",
        "        merges = [tuple(merge.split()) for merge in merges]\n",
        "        vocab = list(bytes_to_unicode().values())\n",
        "        vocab = vocab + [v+'</w>' for v in vocab]\n",
        "        for merge in merges:\n",
        "            vocab.append(''.join(merge))\n",
        "        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
        "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
        "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n",
        "        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token+'</w>'\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                    new_word.append(first+second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = ' '.join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        bpe_tokens = []\n",
        "        text = whitespace_clean(basic_clean(text)).lower()\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        text = ''.join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n",
        "        return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W8ARJVqBJXs"
      },
      "source": [
        "### Task 4:  Semantic search on images retrived from Unsplash\n",
        "**Note:** I would strongly encourage you to sign in to Unsplash https://unsplash.com/developers and get your acess key key and replace in the cell below to run through this demo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6aKLRBtVA9S"
      },
      "source": [
        "unspash_original_api = \"https://api.unsplash.com/search/photos?client_id={access_key}&query={query}&per_page={numresults}&page=1\"\n",
        "unsplash_access_key = \"aRfK6aArC_8xh7upDyfwZdA5vQ1ciJceTDiJQp4zKWo\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkfipvAR3GDf"
      },
      "source": [
        "### install iPyPlot to plot images "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EElyF423O8qT"
      },
      "source": [
        "#  Necessary installations\n",
        "!pip install ipyplot==1.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8hDHaYbTNze"
      },
      "source": [
        "### Search Unsplash with the keyword and retrieve top N images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncSKms93VOdZ"
      },
      "source": [
        "import requests\n",
        "import PIL\n",
        "from io import BytesIO\n",
        "import ipyplot\n",
        "\n",
        "search_keyword = \"dog and cat\"\n",
        "\n",
        "no_to_retrieve = 100\n",
        "unsplash_api = unspash_original_api.format(access_key=unsplash_access_key, query=search_keyword, numresults=no_to_retrieve)\n",
        "response = requests.get(unsplash_api)\n",
        "output = response.json()\n",
        "\n",
        "all_images =[]\n",
        "for each in output[\"results\"]:\n",
        "  urls = each[\"urls\"]\n",
        "  imageurl = urls[\"full\"]\n",
        "  response = requests.get(imageurl)\n",
        "  image = PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "  all_images.append(image)\n",
        "\n",
        "print (\"Total no of images retrived: \",len(all_images))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTgQhT5W3Zz8"
      },
      "source": [
        "### Display top images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwb1dCRYZeHI"
      },
      "source": [
        "# plot the top 50 (max) retrived images\n",
        "ipyplot.plot_images(all_images,max_images =50,img_width=150)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auf7hAod3duP"
      },
      "source": [
        "- **[Task 5]()**: Tokenize search keywords into text embeddings\n",
        "- **[Task 6]()**: Tokenize each image in recall set into image embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-OOTtgVanHP"
      },
      "source": [
        "\n",
        "\n",
        "images = [preprocess(im) for im in all_images]\n",
        "image_input = torch.tensor(np.stack(images)).cuda()\n",
        "image_input -= image_mean[:, None, None]\n",
        "image_input /= image_std[:, None, None]\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "tokenizer = SimpleTokenizer()\n",
        "\n",
        "def get_text_features(sentence):\n",
        "  text_tokens = [tokenizer.encode(\"%s \"%(sentence) + \"<|endoftext|>\")]\n",
        "  text_input = torch.zeros(len(text_tokens), model.context_length, dtype=torch.long)\n",
        "  for i, tokens in enumerate(text_tokens):\n",
        "    text_input[i, :len(tokens)] = torch.tensor(tokens)\n",
        "    \n",
        "  text_input = text_input.cuda()\n",
        "  with torch.no_grad():\n",
        "    text_features = model.encode_text(text_input).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  return text_features\n",
        "\n",
        "def get_top_N_semantic_similarity(similarity_list,N):\n",
        "  results = zip(range(len(similarity_list)), similarity_list)\n",
        "  results = sorted(results, key=lambda x: x[1],reverse= True)\n",
        "  top_N_images = []\n",
        "  scores=[]\n",
        "  for index,score in results[:N]:\n",
        "    scores.append(score)\n",
        "    top_N_images.append(all_images[index])\n",
        "  return scores,top_N_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH90h5fSev5d"
      },
      "source": [
        "- **[Task 7]()**: Perform similarity between image embeddings and text \n",
        "- **[Task 8]()**: Sort the results by descending order similarity score\n",
        "- **[Task 9]()**: Display Top N results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arM9-OcpbCYz"
      },
      "source": [
        "#semantic_search_phrase = \"dog staring at cat\"\n",
        "#semantic_search_phrase = \"cat playing with dog\"\n",
        "#semantic_search_phrase = \"dog with human\"\n",
        "#semantic_search_phrase = \"dog sitting on a chair\"\n",
        "semantic_search_phrase = \"dog with a mountain background\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "text_features_extracted = get_text_features(semantic_search_phrase)\n",
        "similarity = text_features_extracted.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "\n",
        "similarity = similarity[0]\n",
        "scores,imgs= get_top_N_semantic_similarity(similarity,N=3)\n",
        "print (\"scores \",scores)\n",
        "ipyplot.plot_images(imgs,img_width=300)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-bFa4Gve41g"
      },
      "source": [
        "semantic_search_phrase = \"kittens and puppies\"\n",
        "text_features_extracted = get_text_features(semantic_search_phrase)\n",
        "similarity = text_features_extracted.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "\n",
        "similarity = similarity[0]\n",
        "scores,imgs= get_top_N_semantic_similarity(similarity,N=3)\n",
        "print (\"scores \",scores)\n",
        "ipyplot.plot_images(imgs,img_width=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnoq3jJxfb9u"
      },
      "source": [
        "semantic_search_phrase = \"dog and cat playing in the snow\"\n",
        "text_features_extracted = get_text_features(semantic_search_phrase)\n",
        "similarity = text_features_extracted.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "\n",
        "similarity = similarity[0]\n",
        "scores,imgs= get_top_N_semantic_similarity(similarity,N=3)\n",
        "print (\"scores \",scores)\n",
        "ipyplot.plot_images(imgs,img_width=300)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}